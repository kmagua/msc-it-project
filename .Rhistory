as.character(sms_corpus_clean[[1]])
getTransformations()
stopwords()
#as.character(sms_corpus[[1]])
#as.character(sms_corpus_clean[[1]])
#2 remove numbers
sms_corpus_clean <- tm_map(sms_corpus_clean, removeNumbers)
sms_raw <- read.csv("naive/sms_spam.csv", stringsAsFactors = FALSE)
sms_raw <- read.csv("naive/sms_spam.csv", stringsAsFactors = FALSE)
#str(sms_raw)
sms_raw$type <- factor(sms_raw$type)
#str(sms_raw)
library(tm)
sms_corpus <- VCorpus(VectorSource(sms_raw$text))
print(sms_corpus)
#Standardize the words, remove cases, punctuations
#1 to lower
sms_corpus_clean <- tm_map(sms_corpus, content_transformer(tolower))
#as.character(sms_corpus[[1]])
#as.character(sms_corpus_clean[[1]])
#2 remove numbers
sms_corpus_clean <- tm_map(sms_corpus_clean, removeNumbers)
#3 remove stop words
sms_corpus_clean <- tm_map(sms_corpus_clean, removeWords, stopwords())
#4 remove punctuations XXXXXX better of replacing punctuation and the strip spaces
sms_corpus_clean <- tm_map(sms_corpus_clean, removePunctuation)
#5 stemming -> reduction of words to root form (learned, learning ->> learn)
#This allows machine learning algorithms to treat the related terms as a
#single concept rather than attempting to learn a pattern for each variant.
library(SnowballC)
wordStem(c("learn", "learned", "learning", "learns"))
wordStem(c("learned", "learning", "learns")) # outputs learn
sms_raw <- read.csv("naive/sms_spam.csv", stringsAsFactors = FALSE)
#str(sms_raw)
sms_raw$type <- factor(sms_raw$type)
#str(sms_raw)
library(tm)
sms_corpus <- VCorpus(VectorSource(sms_raw$text))
print(sms_corpus)
#Standardize the words, remove cases, punctuations
#1 to lower
sms_corpus_clean <- tm_map(sms_corpus, content_transformer(tolower))
#as.character(sms_corpus[[1]])
#as.character(sms_corpus_clean[[1]])
#2 remove numbers
sms_corpus_clean <- tm_map(sms_corpus_clean, removeNumbers)
#3 remove stop words
sms_corpus_clean <- tm_map(sms_corpus_clean, removeWords, stopwords())
#4 remove punctuations XXXXXX better of replacing punctuation and the strip spaces
sms_corpus_clean <- tm_map(sms_corpus_clean, removePunctuation)
#5 stemming -> reduction of words to root form (learned, learning ->> learn)
#This allows machine learning algorithms to treat the related terms as a
#single concept rather than attempting to learn a pattern for each variant.
library(SnowballC)
wordStem(c("learned", "learning", "learns")) # outputs learn
sms_corpus_clean <- tm_map(sms_corpus_clean, stemDocument)
sms_raw <- read.csv("naive/sms_spam.csv", stringsAsFactors = FALSE)
#str(sms_raw)
sms_raw$type <- factor(sms_raw$type)
#str(sms_raw)
library(tm)
sms_corpus <- VCorpus(VectorSource(sms_raw$text))
print(sms_corpus)
#Standardize the words, remove cases, punctuations
#1 to lower
sms_corpus_clean <- tm_map(sms_corpus, content_transformer(tolower))
#as.character(sms_corpus[[1]])
#as.character(sms_corpus_clean[[1]])
#2 remove numbers
sms_corpus_clean <- tm_map(sms_corpus_clean, removeNumbers)
#3 remove stop words
sms_corpus_clean <- tm_map(sms_corpus_clean, removeWords, stopwords())
#4 remove punctuations XXXXXX better of replacing punctuation and the strip spaces
sms_corpus_clean <- tm_map(sms_corpus_clean, removePunctuation)
#5 stemming -> reduction of words to root form (learned, learning ->> learn)
#This allows machine learning algorithms to treat the related terms as a
#single concept rather than attempting to learn a pattern for each variant.
library(SnowballC)
wordStem(c("learned", "learning", "learns")) # outputs learn
sms_corpus_clean <- tm_map(sms_corpus_clean, stemDocument)
#6 strip spaces
sms_corpus_clean <- tm_map(sms_corpus_clean, stripWhitespace)
sms_raw <- read.csv("naive/sms_spam.csv", stringsAsFactors = FALSE)
#str(sms_raw)
sms_raw$type <- factor(sms_raw$type)
#str(sms_raw)
library(tm)
sms_corpus <- VCorpus(VectorSource(sms_raw$text))
print(sms_corpus)
#YYYYYYYYYYYYYYYYYYY Standardize the words, remove cases, punctuations (TRANSFORMATION)
#1 to lower
sms_corpus_clean <- tm_map(sms_corpus, content_transformer(tolower))
#as.character(sms_corpus[[1]])
#as.character(sms_corpus_clean[[1]])
#2 remove numbers
sms_corpus_clean <- tm_map(sms_corpus_clean, removeNumbers)
#3 remove stop words
sms_corpus_clean <- tm_map(sms_corpus_clean, removeWords, stopwords())
#4 remove punctuations XXXXXX better of replacing punctuation and the strip spaces
sms_corpus_clean <- tm_map(sms_corpus_clean, removePunctuation)
#5 stemming -> reduction of words to root form (learned, learning ->> learn)
#This allows machine learning algorithms to treat the related terms as a
#single concept rather than attempting to learn a pattern for each variant.
library(SnowballC)
wordStem(c("learned", "learning", "learns")) # outputs learn
sms_corpus_clean <- tm_map(sms_corpus_clean, stemDocument)
#6 strip spaces
sms_corpus_clean <- tm_map(sms_corpus_clean, stripWhitespace)
#DATA PREPARATION
#tokenization - create DOCUMENT TERM MATRIX in which a rows are messages and columns are words/tokens (value 0/1)
sms_dtm <- DocumentTermMatrix(sms_corpus_clean)
sms_dtm
install.packages('dplyr')
pwd()
get_pwd()
get_wd()
wbcd <- read.csv("Harvard-statsR/msleep_ggplot2.csv", stringsAsFactors = FALSE)
data <- read.csv("Harvard-statsR/msleep_ggplot2.csv", stringsAsFactors = FALSE)
data
class(data)
library(dplyr)
primates <- filter(data, order=='Primates')
nRow(primates)
nrow(primates)
class(primates)
sleep_total <- filter(data, order=='Primates') %>% select(sleep_total)
class(sleep_total)
sleep_total
avg <- unlist(sleep_total) %>% mean(sleep_total)
avg <- unlist(sleep_total) %>% mean()
avg
sleep_total <- filter(data, order=='Primates') %>% summarize()
sleep_total
data <- read.csv("Harvard-statsR/femaleControlsPopulation.csv", stringsAsFactors = FALSE)
x <- unlist( data )
x <- unlist( data ) %>% mean()
mean(x)
data <- read.csv("Harvard-statsR/femaleControlsPopulation.csv", stringsAsFactors = FALSE)
x <- unlist( data )
mean(x)
abs(mean(x))
data <- read.csv("Harvard-statsR/femaleControlsPopulation.csv", stringsAsFactors = FALSE)
x <- unlist( data )
m <- mean(x)
set.seed(1)
sm <- mean(sample(x,5))
abs(m-sm)
data <- read.csv("Harvard-statsR/femaleControlsPopulation.csv", stringsAsFactors = FALSE)
x <- unlist( data )
m <- mean(x)
set.seed(1)
sm <- mean(sample(x,5))
abs(m-sm)
data <- read.csv("Harvard-statsR/femaleControlsPopulation.csv", stringsAsFactors = FALSE)
x <- unlist( data )
m <- mean(x)
set.seed(1)
sm <- mean(sample(x,5))
abs(m-sm)
data <- read.csv("Harvard-statsR/femaleControlsPopulation.csv", stringsAsFactors = FALSE)
x <- unlist( data )
m <- mean(x)
set.seed(5)
sm <- mean(sample(x,5))
abs(m-sm)
install.packages("wordcloud")
sms_raw <- read.csv("naive/sms_spam.csv", stringsAsFactors = FALSE)
#str(sms_raw)
sms_raw$type <- factor(sms_raw$type)
#str(sms_raw)
library(tm)
sms_corpus <- VCorpus(VectorSource(sms_raw$text))
print(sms_corpus)
#YYYYYYYYYYYYYYYYYYY Standardize the words, remove cases, punctuations (TRANSFORMATION)
#1 to lower
sms_corpus_clean <- tm_map(sms_corpus, content_transformer(tolower))
#as.character(sms_corpus[[1]])
#as.character(sms_corpus_clean[[1]])
#2 remove numbers
sms_corpus_clean <- tm_map(sms_corpus_clean, removeNumbers)
#3 remove stop words
sms_corpus_clean <- tm_map(sms_corpus_clean, removeWords, stopwords())
#4 remove punctuations XXXXXX better of replacing punctuation and the strip spaces
sms_corpus_clean <- tm_map(sms_corpus_clean, removePunctuation)
#5 stemming -> reduction of words to root form (learned, learning ->> learn)
#This allows machine learning algorithms to treat the related terms as a
#single concept rather than attempting to learn a pattern for each variant.
library(SnowballC)
wordStem(c("learned", "learning", "learns")) # outputs learn
sms_corpus_clean <- tm_map(sms_corpus_clean, stemDocument)
#6 strip spaces
sms_corpus_clean <- tm_map(sms_corpus_clean, stripWhitespace)
#DATA PREPARATION
#1. tokenization - create DOCUMENT TERM MATRIX in which a rows are messages and columns are words/tokens (value 0/1)
sms_dtm <- DocumentTermMatrix(sms_corpus_clean)
sms_dtm
# 2. Data preparation – creating training and test datasets (the data is already ordered randomly)
sms_dtm_train <- sms_dtm[1:4169, ]
sms_dtm_test <- sms_dtm[4170:5559, ]
sms_train_labels <- sms_raw[1:4169, ]$type
sms_test_labels <- sms_raw[4170:5559, ]$type
#Visualizing text data – word clouds (way to visually depict the frequency at which words appear intext data)
library(wordcloud)
wordcloud(sms_corpus_clean, min.freq = 50, random.order = FALSE)
spam <- subset(sms_raw, type == "spam")
wordcloud(spam$text, max.words = 40, scale = c(3, 0.5))
wordcloud(ham$text, max.words = 40, scale = c(3, 0.5))
spam <- subset(sms_raw, type == "spam")
ham <- subset(sms_raw, type == "ham")
wordcloud(spam$text, max.words = 40, scale = c(3, 0.5))
wordcloud(ham$text, max.words = 40, scale = c(3, 0.5))
wordcloud(spam$text, max.words = 40, scale = c(3, 0.5))
installed.packages('e1071')
install.packages('e1071')
install.packages('e1071')
sms_raw <- read.csv("naive/sms_spam.csv", stringsAsFactors = FALSE)
#str(sms_raw)
sms_raw$type <- factor(sms_raw$type)
#str(sms_raw)
library(tm)
sms_corpus <- VCorpus(VectorSource(sms_raw$text))
print(sms_corpus)
#YYYYYYYYYYYYYYYYYYY Standardize the words, remove cases, punctuations (TRANSFORMATION)
#1 to lower
sms_corpus_clean <- tm_map(sms_corpus, content_transformer(tolower))
#as.character(sms_corpus[[1]])
#as.character(sms_corpus_clean[[1]])
#2 remove numbers
sms_corpus_clean <- tm_map(sms_corpus_clean, removeNumbers)
#3 remove stop words
sms_corpus_clean <- tm_map(sms_corpus_clean, removeWords, stopwords())
#4 remove punctuations XXXXXX better of replacing punctuation and the strip spaces
sms_corpus_clean <- tm_map(sms_corpus_clean, removePunctuation)
#5 stemming -> reduction of words to root form (learned, learning ->> learn)
#This allows machine learning algorithms to treat the related terms as a
#single concept rather than attempting to learn a pattern for each variant.
library(SnowballC)
wordStem(c("learned", "learning", "learns")) # outputs learn
sms_corpus_clean <- tm_map(sms_corpus_clean, stemDocument)
#6 strip spaces
sms_corpus_clean <- tm_map(sms_corpus_clean, stripWhitespace)
#DATA PREPARATION
#1. tokenization - create DOCUMENT TERM MATRIX in which a rows are messages and columns are words/tokens (value 0/1)
sms_dtm <- DocumentTermMatrix(sms_corpus_clean)
sms_dtm
# 2. Data preparation – creating training and test datasets (the data is already ordered randomly)
sms_dtm_train <- sms_dtm[1:4169, ]
sms_dtm_test <- sms_dtm[4170:5559, ]
sms_train_labels <- sms_raw[1:4169, ]$type
sms_test_labels <- sms_raw[4170:5559, ]$type
#Visualizing text data – word clouds (way to visually depict the frequency at which words appear intext data)
library(wordcloud)
wordcloud(sms_corpus_clean, min.freq = 50, random.order = FALSE)
#compare word clouds of two categories
spam <- subset(sms_raw, type == "spam")
ham <- subset(sms_raw, type == "ham")
wordcloud(spam$text, max.words = 40, scale = c(3, 0.5))
wordcloud(ham$text, max.words = 40, scale = c(3, 0.5))
#Data preparation – creating indicator features for frequent words
sms_freq_words <- findFreqTerms(sms_dtm_train, 5)
sms_dtm_freq_train<- sms_dtm_train[ , sms_freq_words]
sms_dtm_freq_test <- sms_dtm_test[ , sms_freq_words]
#convert numerical to categorical data for NB
convert_counts <- function(x) {
x <- ifelse(x > 0, "Yes", "No")
}
sms_train <- apply(sms_dtm_freq_train, MARGIN = 2, convert_counts)
sms_test <- apply(sms_dtm_freq_test, MARGIN = 2, convert_counts)
#STEP 3 – TRAINING a model on the data
library(e1071)
sms_classifier <- naiveBayes(sms_train, sms_train_labels)
sms_test_pred <- predict(sms_classifier, sms_test)
library(gmodels)
CrossTable(sms_test_pred, sms_test_labels, prop.chisq = FALSE, prop.t = FALSE, dnn = c('predicted', 'actual'))
#STEP 5 improving model performance
# Laplace estimator allows words that appeared in zero spam or zero ham
# messages to have an indisputable say in the classification process.
#Laplace estimator essentially adds a small number to each of the counts in the frequency table,
#which ensures that each feature has a nonzero probability of occurring with each class.
#Usually Laplace estimator is set to 1, which ensures that each class-feature combination is found in the data at least once:
sms_classifier2 <- naiveBayes(sms_train, sms_train_labels, laplace = 7)
sms_test_pred2 <- predict(sms_classifier2, sms_test)
CrossTable(sms_test_pred2, sms_test_labels, prop.chisq = FALSE, prop.t = FALSE, prop.r = FALSE, dnn = c('predicted', 'actual'))
#STEP 5 improving model performance
# Laplace estimator allows words that appeared in zero spam or zero ham
# messages to have an indisputable say in the classification process.
#Laplace estimator essentially adds a small number to each of the counts in the frequency table,
#which ensures that each feature has a nonzero probability of occurring with each class.
#Usually Laplace estimator is set to 1, which ensures that each class-feature combination is found in the data at least once:
sms_classifier2 <- naiveBayes(sms_train, sms_train_labels, laplace = 1)
sms_test_pred2 <- predict(sms_classifier2, sms_test)
CrossTable(sms_test_pred2, sms_test_labels, prop.chisq = FALSE, prop.t = FALSE, prop.r = FALSE, dnn = c('predicted', 'actual'))
credit <- read.csv('decision_tree/credit.csv')
str(credit)
summary(credit$age)
table(credit$default)
credit <- read.csv('decision_tree/credit.csv')
str(credit)
summary(credit$age)
table(credit$default)
credit <- read.csv('decision_tree/credit.csv')
str(credit)
summary(credit$age)
table(credit$default)
table(credit$checking_balance)
table(credit$savings_balance)
# Randomize train/test data select
set.seed(123)
sample < sample(1000, 900)
train_sample <- sample(1000, 900)
train_sample
credit_train <- credit[train_sample,]
credit_test <- credit[-train_sample,]
prop.table(table(credit_train$default))
prop.table(table(credit_test$default))
# Randomize train/test data select
set.seed(123)
train_sample <- sample(1000, 900)
credit_train <- credit[train_sample,]
credit_test <- credit[-train_sample,]
prop.table(table(credit_train$default)) #### should be about 30% each for defaulters
prop.table(table(credit_test$default))
install.packages("C50")
#prop.table(table(credit_train$default)) #### should be about 30% each for defaulters
#prop.table(table(credit_test$default))
library(C50)
library(C50)
install.packages("C50")
library(C50)
#prop.table(table(credit_train$default)) #### should be about 30% each for defaulters
#prop.table(table(credit_test$default))
library(C50)
credit <- read.csv('decision_tree/credit.csv')
#1 exlore data
#str(credit)
#summary(credit$age)
#table(credit$default)
#table(credit$checking_balance)
#table(credit$savings_balance)
# Randomize train/test data select
set.seed(123)
train_sample <- sample(1000, 900)
credit_train <- credit[train_sample,]
credit_test <- credit[-train_sample,]
#prop.table(table(credit_train$default)) #### should be about 30% each for defaulters
#prop.table(table(credit_test$default))
library(C50)
data(credit)
table(credit$default)
credit_model <- C5.0(credit_train[-17], credit_train$default)
credit_model
#credit_model tree details
summary(credit_model)
library(MASS)
data("Pima.tr")
mushrooms <- read.csv("decision_tree/rules/mushrooms.csv", stringsAsFactors = TRUE)
str(mushrooms)
mushrooms$veil_type <- NULL #remove this as all the values are the same
str(mushrooms)
table(mushrooms$type)
library(RWeka)
install.packages('RWeka')
library(RWeka)
install.packages('RWeka')
library(RWeka)
library(RWeka)
library(RWeka)
mushroom_1R <- OneR(type ~ ., data = mushrooms)
mushroom_1R
summary(mushroom_1R)
#improving performance
#using a more sophisticated rule learner
mushroom_JRip <- JRip(type ~ ., data = mushrooms)
mushroom_JRip
launch <- read.csv("lr/challenger.csv")
r <- cov(launch$temperature, launch$distress_ct) / (sd(launch$temperature) * sd(launch$distress_ct))
r
reg <- function(y, x) {
x <- as.matrix(x)
x <- cbind(Intercept = 1, x)
b <- solve(t(x) %*% x) %*% t(x) %*% y
colnames(b) <- "estimate"
print(b)
}
str(launch)
reg(y = launch$distress_ct, x = launch[2])
reg(y = launch$distress_ct, x = launch[2:4])
#  XXXXXXXXXXX Example - Example – predicting medical expenses using linear regression XXXXXXXXXXXXXX
insurance <- read.csv("lr/insurance.csv", stringsAsFactors = TRUE)
str(insurance)
summary(insurance$expenses)
insurance <- read.csv("lr/insurance.csv", stringsAsFactors = TRUE)
summary(insurance$expenses)
str(insurance)
summary(insurance$charges)
hist(insurance$charges)
table(insurance$region)
#explore data -  Correlation Matrix
cor(insurance[c("age", "bmi", "children", "expenses")])
#explore data -  Correlation Matrix
cor(insurance[c("age", "bmi", "children", "charges")])
#scatter plot for pairs
pairs(insurance[c("age", "bmi", "children", "expenses")])
#scatter plot for pairs
pairs(insurance[c("age", "bmi", "children", "charges")])
install.packages("psych")
#Training a model
ins_model <- lm(expenses ~ age + children + bmi + sex + smoker + region, data = insurance)
#launch <- read.csv("lr/challenger.csv")
#r <- cov(launch$temperature, launch$distress_ct) / (sd(launch$temperature) * sd(launch$distress_ct))
#reg <- function(y, x) {
#    x <- as.matrix(x)
#    x <- cbind(Intercept = 1, x)
#    b <- solve(t(x) %*% x) %*% t(x) %*% y
#    colnames(b) <- "estimate"
#    print(b)
#}
## Simple LR
#reg(y = launch$distress_ct, x = launch[2])
##multiple LR
#reg(y = launch$distress_ct, x = launch[2:4])
#  XXXXXXXXXXX Example - Example – predicting medical expenses using linear regression XXXXXXXXXXXXXX
insurance <- read.csv("lr/insurance.csv", stringsAsFactors = TRUE)
#explore data -  Correlation Matrix
cor(insurance[c("age", "bmi", "children", "charges")])
#scatter plot for pairs
pairs(insurance[c("age", "bmi", "children", "charges")])
#Training a model
ins_model <- lm(expenses ~ age + children + bmi + sex + smoker + region, data = insurance)
# ins_model <- lm(expenses ~ ., data = insurance) ##### similar to above
#Training a model
ins_model <- lm(charges ~ age + children + bmi + sex + smoker + region, data = insurance)
ins_model
summary(ins_model)
# ins_model <- lm(charges ~ ., data = insurance) ##### similar to above
#view model performance
summary(ins_model)
## XXXXXXXXXXXXX improving the model XXXXXXXXXXXXXX
#adding non-linear relationships
insurance$age2 <- insurance$age^2
#transform numeric to binary ### All obese cases to have 1 0 otherwise
insurance$bmi30 <- ifelse(insurance$bmi >= 30, 1, 0)
#Model specification – adding interaction effects #some group of features could have combined effect
#bmi30*smoker
ins_model2 <- lm(charges ~ age + age2 + children + bmi + sex + bmi30*smoker + region, data = insurance)
summary(ins_model2)
wine <- read.csv("lr/whitewines.csv")
#divide the data into train/test
wine_train <- wine[1:3750, ]
wine_test <- wine[3751:4898, ]
library(rpart)
#training a model
m.rpart <- rpart(quality ~ ., data = wine_train)
m.rpart
chd <- read.csv("Project/chronic_kidney_disease_full.csv")
str(chd)
chd[ chd == "?" ] <- NA
str(chd)
chd <- read.csv("Project/chronic_kidney_disease_full.csv", na.strings = c("?", "\t?"))
str(chd)
chd <- read.csv("Project/chronic_kidney_disease_full.csv", na.strings = c("?", "\t?"), stringsAsFactors = TRUE)
str(chd)
ckd <- read.csv("Project/chronic_kidney_disease_full.csv", na.strings = c("?", "\t?"), stringsAsFactors = TRUE)
#chd[ chd == "?" ] <- NA
ckd
ckd <- read.csv("Project/chronic_kidney_disease_full.csv", na.strings = c("?", "\t?"), stringsAsFactors = TRUE)
str(ckd)
ckd <- read.csv("Project/chronic_kidney_disease_full.csv", na.strings = c("?", "\t?"), stringsAsFactors = TRUE)
str(ckd)
ckd <- read.csv("Project/chronic_kidney_disease_full.csv", na.strings = c("?", "\t?"), stringsAsFactors = TRUE)
str(ckd)
ckd <- read.csv("Project/chronic_kidney_disease.csv", na.strings = c("?", "\t?"), stringsAsFactors = TRUE)
str(ckd)
ckd <- read.csv("Project/chronic_kidney_disease.csv", na.strings = c("?", "\t?"), stringsAsFactors = TRUE)
str(ckd)
ckd <- read.csv("Project/chronic_kidney_disease.csv", na.strings = c("?", "\t?"), stringsAsFactors = TRUE)
str(ckd)
#any_na(ckd)
numeric_columns <- unlist(lapply(ckd, is.numeric))
numeric_columns
concrete <- read.csv("ann/concrete.csv")
#Neural networks work best when the input data are scaled to a narrow range around zero,
normalize <- function(x) {
return((x - min(x)) / (max(x) - min(x)))
}
concrete_norm <- as.data.frame(lapply(concrete, normalize))
concrete_norm
ckd <- read.csv("original_complete_cases.csv", stringsAsFactors = TRUE)
str(ckd)
#any_na(ckd)
numeric_cols <- sapply(ckd, is.numeric)
anorexia[numeric_cols] <- lapply(anorexia[numeric_cols], scale)
#any_na(ckd)
numeric_cols <- sapply(ckd, is.numeric)
ckd[numeric_cols] <- lapply(anorexia[numeric_cols], scale)
ckd[numeric_cols] <- lapply(ckd[numeric_cols], scale)
str(ckd)
write.csv(numeric_columns, file = "original_complete_cases_numeric_scaled.csv",row.names=FALSE)
setwd('Project')
write.csv(numeric_columns, file = "original_complete_cases_numeric_scaled.csv",row.names=FALSE)
ckd <- read.csv("original_complete_cases.csv", stringsAsFactors = TRUE)
str(ckd)
#any_na(ckd)
numeric_cols <- sapply(ckd, is.numeric)
ckd[numeric_cols] <- lapply(ckd[numeric_cols], scale)
numeric_columns <- unlist(lapply(ckd, is.numeric))
write.csv(numeric_columns, file = "original_complete_cases_numeric_scaled.csv",row.names=FALSE)
numeric_columns <- unlist(lapply(ckd, is.numeric))
#any_na(ckd)
numeric_cols <- sapply(ckd, is.numeric)
ckd[numeric_cols] <- lapply(ckd[numeric_cols], scale)
write.csv(ckd, file = "original_complete_cases_numeric_scaled.csv",row.names=FALSE)
