library(SnowballC)
wordStem(c("learned", "learning", "learns")) # outputs learn
sms_corpus_clean <- tm_map(sms_corpus_clean, stemDocument)
#6 strip spaces
sms_corpus_clean <- tm_map(sms_corpus_clean, stripWhitespace)
#DATA PREPARATION
#tokenization - create DOCUMENT TERM MATRIX in which a rows are messages and columns are words/tokens (value 0/1)
sms_dtm <- DocumentTermMatrix(sms_corpus_clean)
sms_dtm
install.packages('dplyr')
pwd()
get_pwd()
get_wd()
wbcd <- read.csv("Harvard-statsR/msleep_ggplot2.csv", stringsAsFactors = FALSE)
data <- read.csv("Harvard-statsR/msleep_ggplot2.csv", stringsAsFactors = FALSE)
data
class(data)
library(dplyr)
primates <- filter(data, order=='Primates')
nRow(primates)
nrow(primates)
class(primates)
sleep_total <- filter(data, order=='Primates') %>% select(sleep_total)
class(sleep_total)
sleep_total
avg <- unlist(sleep_total) %>% mean(sleep_total)
avg <- unlist(sleep_total) %>% mean()
avg
sleep_total <- filter(data, order=='Primates') %>% summarize()
sleep_total
data <- read.csv("Harvard-statsR/femaleControlsPopulation.csv", stringsAsFactors = FALSE)
x <- unlist( data )
x <- unlist( data ) %>% mean()
mean(x)
data <- read.csv("Harvard-statsR/femaleControlsPopulation.csv", stringsAsFactors = FALSE)
x <- unlist( data )
mean(x)
abs(mean(x))
data <- read.csv("Harvard-statsR/femaleControlsPopulation.csv", stringsAsFactors = FALSE)
x <- unlist( data )
m <- mean(x)
set.seed(1)
sm <- mean(sample(x,5))
abs(m-sm)
data <- read.csv("Harvard-statsR/femaleControlsPopulation.csv", stringsAsFactors = FALSE)
x <- unlist( data )
m <- mean(x)
set.seed(1)
sm <- mean(sample(x,5))
abs(m-sm)
data <- read.csv("Harvard-statsR/femaleControlsPopulation.csv", stringsAsFactors = FALSE)
x <- unlist( data )
m <- mean(x)
set.seed(1)
sm <- mean(sample(x,5))
abs(m-sm)
data <- read.csv("Harvard-statsR/femaleControlsPopulation.csv", stringsAsFactors = FALSE)
x <- unlist( data )
m <- mean(x)
set.seed(5)
sm <- mean(sample(x,5))
abs(m-sm)
install.packages("wordcloud")
sms_raw <- read.csv("naive/sms_spam.csv", stringsAsFactors = FALSE)
#str(sms_raw)
sms_raw$type <- factor(sms_raw$type)
#str(sms_raw)
library(tm)
sms_corpus <- VCorpus(VectorSource(sms_raw$text))
print(sms_corpus)
#YYYYYYYYYYYYYYYYYYY Standardize the words, remove cases, punctuations (TRANSFORMATION)
#1 to lower
sms_corpus_clean <- tm_map(sms_corpus, content_transformer(tolower))
#as.character(sms_corpus[[1]])
#as.character(sms_corpus_clean[[1]])
#2 remove numbers
sms_corpus_clean <- tm_map(sms_corpus_clean, removeNumbers)
#3 remove stop words
sms_corpus_clean <- tm_map(sms_corpus_clean, removeWords, stopwords())
#4 remove punctuations XXXXXX better of replacing punctuation and the strip spaces
sms_corpus_clean <- tm_map(sms_corpus_clean, removePunctuation)
#5 stemming -> reduction of words to root form (learned, learning ->> learn)
#This allows machine learning algorithms to treat the related terms as a
#single concept rather than attempting to learn a pattern for each variant.
library(SnowballC)
wordStem(c("learned", "learning", "learns")) # outputs learn
sms_corpus_clean <- tm_map(sms_corpus_clean, stemDocument)
#6 strip spaces
sms_corpus_clean <- tm_map(sms_corpus_clean, stripWhitespace)
#DATA PREPARATION
#1. tokenization - create DOCUMENT TERM MATRIX in which a rows are messages and columns are words/tokens (value 0/1)
sms_dtm <- DocumentTermMatrix(sms_corpus_clean)
sms_dtm
# 2. Data preparation – creating training and test datasets (the data is already ordered randomly)
sms_dtm_train <- sms_dtm[1:4169, ]
sms_dtm_test <- sms_dtm[4170:5559, ]
sms_train_labels <- sms_raw[1:4169, ]$type
sms_test_labels <- sms_raw[4170:5559, ]$type
#Visualizing text data – word clouds (way to visually depict the frequency at which words appear intext data)
library(wordcloud)
wordcloud(sms_corpus_clean, min.freq = 50, random.order = FALSE)
spam <- subset(sms_raw, type == "spam")
wordcloud(spam$text, max.words = 40, scale = c(3, 0.5))
wordcloud(ham$text, max.words = 40, scale = c(3, 0.5))
spam <- subset(sms_raw, type == "spam")
ham <- subset(sms_raw, type == "ham")
wordcloud(spam$text, max.words = 40, scale = c(3, 0.5))
wordcloud(ham$text, max.words = 40, scale = c(3, 0.5))
wordcloud(spam$text, max.words = 40, scale = c(3, 0.5))
installed.packages('e1071')
install.packages('e1071')
install.packages('e1071')
sms_raw <- read.csv("naive/sms_spam.csv", stringsAsFactors = FALSE)
#str(sms_raw)
sms_raw$type <- factor(sms_raw$type)
#str(sms_raw)
library(tm)
sms_corpus <- VCorpus(VectorSource(sms_raw$text))
print(sms_corpus)
#YYYYYYYYYYYYYYYYYYY Standardize the words, remove cases, punctuations (TRANSFORMATION)
#1 to lower
sms_corpus_clean <- tm_map(sms_corpus, content_transformer(tolower))
#as.character(sms_corpus[[1]])
#as.character(sms_corpus_clean[[1]])
#2 remove numbers
sms_corpus_clean <- tm_map(sms_corpus_clean, removeNumbers)
#3 remove stop words
sms_corpus_clean <- tm_map(sms_corpus_clean, removeWords, stopwords())
#4 remove punctuations XXXXXX better of replacing punctuation and the strip spaces
sms_corpus_clean <- tm_map(sms_corpus_clean, removePunctuation)
#5 stemming -> reduction of words to root form (learned, learning ->> learn)
#This allows machine learning algorithms to treat the related terms as a
#single concept rather than attempting to learn a pattern for each variant.
library(SnowballC)
wordStem(c("learned", "learning", "learns")) # outputs learn
sms_corpus_clean <- tm_map(sms_corpus_clean, stemDocument)
#6 strip spaces
sms_corpus_clean <- tm_map(sms_corpus_clean, stripWhitespace)
#DATA PREPARATION
#1. tokenization - create DOCUMENT TERM MATRIX in which a rows are messages and columns are words/tokens (value 0/1)
sms_dtm <- DocumentTermMatrix(sms_corpus_clean)
sms_dtm
# 2. Data preparation – creating training and test datasets (the data is already ordered randomly)
sms_dtm_train <- sms_dtm[1:4169, ]
sms_dtm_test <- sms_dtm[4170:5559, ]
sms_train_labels <- sms_raw[1:4169, ]$type
sms_test_labels <- sms_raw[4170:5559, ]$type
#Visualizing text data – word clouds (way to visually depict the frequency at which words appear intext data)
library(wordcloud)
wordcloud(sms_corpus_clean, min.freq = 50, random.order = FALSE)
#compare word clouds of two categories
spam <- subset(sms_raw, type == "spam")
ham <- subset(sms_raw, type == "ham")
wordcloud(spam$text, max.words = 40, scale = c(3, 0.5))
wordcloud(ham$text, max.words = 40, scale = c(3, 0.5))
#Data preparation – creating indicator features for frequent words
sms_freq_words <- findFreqTerms(sms_dtm_train, 5)
sms_dtm_freq_train<- sms_dtm_train[ , sms_freq_words]
sms_dtm_freq_test <- sms_dtm_test[ , sms_freq_words]
#convert numerical to categorical data for NB
convert_counts <- function(x) {
x <- ifelse(x > 0, "Yes", "No")
}
sms_train <- apply(sms_dtm_freq_train, MARGIN = 2, convert_counts)
sms_test <- apply(sms_dtm_freq_test, MARGIN = 2, convert_counts)
#STEP 3 – TRAINING a model on the data
library(e1071)
sms_classifier <- naiveBayes(sms_train, sms_train_labels)
sms_test_pred <- predict(sms_classifier, sms_test)
library(gmodels)
CrossTable(sms_test_pred, sms_test_labels, prop.chisq = FALSE, prop.t = FALSE, dnn = c('predicted', 'actual'))
#STEP 5 improving model performance
# Laplace estimator allows words that appeared in zero spam or zero ham
# messages to have an indisputable say in the classification process.
#Laplace estimator essentially adds a small number to each of the counts in the frequency table,
#which ensures that each feature has a nonzero probability of occurring with each class.
#Usually Laplace estimator is set to 1, which ensures that each class-feature combination is found in the data at least once:
sms_classifier2 <- naiveBayes(sms_train, sms_train_labels, laplace = 7)
sms_test_pred2 <- predict(sms_classifier2, sms_test)
CrossTable(sms_test_pred2, sms_test_labels, prop.chisq = FALSE, prop.t = FALSE, prop.r = FALSE, dnn = c('predicted', 'actual'))
#STEP 5 improving model performance
# Laplace estimator allows words that appeared in zero spam or zero ham
# messages to have an indisputable say in the classification process.
#Laplace estimator essentially adds a small number to each of the counts in the frequency table,
#which ensures that each feature has a nonzero probability of occurring with each class.
#Usually Laplace estimator is set to 1, which ensures that each class-feature combination is found in the data at least once:
sms_classifier2 <- naiveBayes(sms_train, sms_train_labels, laplace = 1)
sms_test_pred2 <- predict(sms_classifier2, sms_test)
CrossTable(sms_test_pred2, sms_test_labels, prop.chisq = FALSE, prop.t = FALSE, prop.r = FALSE, dnn = c('predicted', 'actual'))
credit <- read.csv('decision_tree/credit.csv')
str(credit)
summary(credit$age)
table(credit$default)
credit <- read.csv('decision_tree/credit.csv')
str(credit)
summary(credit$age)
table(credit$default)
credit <- read.csv('decision_tree/credit.csv')
str(credit)
summary(credit$age)
table(credit$default)
table(credit$checking_balance)
table(credit$savings_balance)
# Randomize train/test data select
set.seed(123)
sample < sample(1000, 900)
train_sample <- sample(1000, 900)
train_sample
credit_train <- credit[train_sample,]
credit_test <- credit[-train_sample,]
prop.table(table(credit_train$default))
prop.table(table(credit_test$default))
# Randomize train/test data select
set.seed(123)
train_sample <- sample(1000, 900)
credit_train <- credit[train_sample,]
credit_test <- credit[-train_sample,]
prop.table(table(credit_train$default)) #### should be about 30% each for defaulters
prop.table(table(credit_test$default))
install.packages("C50")
#prop.table(table(credit_train$default)) #### should be about 30% each for defaulters
#prop.table(table(credit_test$default))
library(C50)
library(C50)
install.packages("C50")
library(C50)
#prop.table(table(credit_train$default)) #### should be about 30% each for defaulters
#prop.table(table(credit_test$default))
library(C50)
credit <- read.csv('decision_tree/credit.csv')
#1 exlore data
#str(credit)
#summary(credit$age)
#table(credit$default)
#table(credit$checking_balance)
#table(credit$savings_balance)
# Randomize train/test data select
set.seed(123)
train_sample <- sample(1000, 900)
credit_train <- credit[train_sample,]
credit_test <- credit[-train_sample,]
#prop.table(table(credit_train$default)) #### should be about 30% each for defaulters
#prop.table(table(credit_test$default))
library(C50)
data(credit)
table(credit$default)
credit_model <- C5.0(credit_train[-17], credit_train$default)
credit_model
#credit_model tree details
summary(credit_model)
library(MASS)
data("Pima.tr")
mushrooms <- read.csv("decision_tree/rules/mushrooms.csv", stringsAsFactors = TRUE)
str(mushrooms)
mushrooms$veil_type <- NULL #remove this as all the values are the same
str(mushrooms)
table(mushrooms$type)
library(RWeka)
install.packages('RWeka')
library(RWeka)
install.packages('RWeka')
library(RWeka)
library(RWeka)
library(RWeka)
mushroom_1R <- OneR(type ~ ., data = mushrooms)
mushroom_1R
summary(mushroom_1R)
#improving performance
#using a more sophisticated rule learner
mushroom_JRip <- JRip(type ~ ., data = mushrooms)
mushroom_JRip
launch <- read.csv("lr/challenger.csv")
r <- cov(launch$temperature, launch$distress_ct) / (sd(launch$temperature) * sd(launch$distress_ct))
r
reg <- function(y, x) {
x <- as.matrix(x)
x <- cbind(Intercept = 1, x)
b <- solve(t(x) %*% x) %*% t(x) %*% y
colnames(b) <- "estimate"
print(b)
}
str(launch)
reg(y = launch$distress_ct, x = launch[2])
reg(y = launch$distress_ct, x = launch[2:4])
#  XXXXXXXXXXX Example - Example – predicting medical expenses using linear regression XXXXXXXXXXXXXX
insurance <- read.csv("lr/insurance.csv", stringsAsFactors = TRUE)
str(insurance)
summary(insurance$expenses)
insurance <- read.csv("lr/insurance.csv", stringsAsFactors = TRUE)
summary(insurance$expenses)
str(insurance)
summary(insurance$charges)
hist(insurance$charges)
table(insurance$region)
#explore data -  Correlation Matrix
cor(insurance[c("age", "bmi", "children", "expenses")])
#explore data -  Correlation Matrix
cor(insurance[c("age", "bmi", "children", "charges")])
#scatter plot for pairs
pairs(insurance[c("age", "bmi", "children", "expenses")])
#scatter plot for pairs
pairs(insurance[c("age", "bmi", "children", "charges")])
install.packages("psych")
#Training a model
ins_model <- lm(expenses ~ age + children + bmi + sex + smoker + region, data = insurance)
#launch <- read.csv("lr/challenger.csv")
#r <- cov(launch$temperature, launch$distress_ct) / (sd(launch$temperature) * sd(launch$distress_ct))
#reg <- function(y, x) {
#    x <- as.matrix(x)
#    x <- cbind(Intercept = 1, x)
#    b <- solve(t(x) %*% x) %*% t(x) %*% y
#    colnames(b) <- "estimate"
#    print(b)
#}
## Simple LR
#reg(y = launch$distress_ct, x = launch[2])
##multiple LR
#reg(y = launch$distress_ct, x = launch[2:4])
#  XXXXXXXXXXX Example - Example – predicting medical expenses using linear regression XXXXXXXXXXXXXX
insurance <- read.csv("lr/insurance.csv", stringsAsFactors = TRUE)
#explore data -  Correlation Matrix
cor(insurance[c("age", "bmi", "children", "charges")])
#scatter plot for pairs
pairs(insurance[c("age", "bmi", "children", "charges")])
#Training a model
ins_model <- lm(expenses ~ age + children + bmi + sex + smoker + region, data = insurance)
# ins_model <- lm(expenses ~ ., data = insurance) ##### similar to above
#Training a model
ins_model <- lm(charges ~ age + children + bmi + sex + smoker + region, data = insurance)
ins_model
summary(ins_model)
# ins_model <- lm(charges ~ ., data = insurance) ##### similar to above
#view model performance
summary(ins_model)
## XXXXXXXXXXXXX improving the model XXXXXXXXXXXXXX
#adding non-linear relationships
insurance$age2 <- insurance$age^2
#transform numeric to binary ### All obese cases to have 1 0 otherwise
insurance$bmi30 <- ifelse(insurance$bmi >= 30, 1, 0)
#Model specification – adding interaction effects #some group of features could have combined effect
#bmi30*smoker
ins_model2 <- lm(charges ~ age + age2 + children + bmi + sex + bmi30*smoker + region, data = insurance)
summary(ins_model2)
wine <- read.csv("lr/whitewines.csv")
#divide the data into train/test
wine_train <- wine[1:3750, ]
wine_test <- wine[3751:4898, ]
library(rpart)
#training a model
m.rpart <- rpart(quality ~ ., data = wine_train)
m.rpart
chd <- read.csv("Project/chronic_kidney_disease_full.csv")
str(chd)
chd[ chd == "?" ] <- NA
str(chd)
chd <- read.csv("Project/chronic_kidney_disease_full.csv", na.strings = c("?", "\t?"))
str(chd)
chd <- read.csv("Project/chronic_kidney_disease_full.csv", na.strings = c("?", "\t?"), stringsAsFactors = TRUE)
str(chd)
ckd <- read.csv("Project/chronic_kidney_disease_full.csv", na.strings = c("?", "\t?"), stringsAsFactors = TRUE)
#chd[ chd == "?" ] <- NA
ckd
ckd <- read.csv("Project/chronic_kidney_disease_full.csv", na.strings = c("?", "\t?"), stringsAsFactors = TRUE)
str(ckd)
ckd <- read.csv("Project/chronic_kidney_disease_full.csv", na.strings = c("?", "\t?"), stringsAsFactors = TRUE)
str(ckd)
ckd <- read.csv("Project/chronic_kidney_disease_full.csv", na.strings = c("?", "\t?"), stringsAsFactors = TRUE)
str(ckd)
ckd <- read.csv("Project/chronic_kidney_disease.csv", na.strings = c("?", "\t?"), stringsAsFactors = TRUE)
str(ckd)
ckd <- read.csv("Project/chronic_kidney_disease.csv", na.strings = c("?", "\t?"), stringsAsFactors = TRUE)
str(ckd)
ckd <- read.csv("Project/chronic_kidney_disease.csv", na.strings = c("?", "\t?"), stringsAsFactors = TRUE)
str(ckd)
#any_na(ckd)
numeric_columns <- unlist(lapply(ckd, is.numeric))
numeric_columns
install.packages('VIM')
install.packages(car)
install.packages('car')
data <- read.csv('transformed_imputed_data.csv')
data <- randomize_data(data, "randomized_imputed_records.csv")
setwd('Project')
#the Neural Network Package
library(neuralnet)
#Caret is used for creating the confusion matrix
library(caret)
#K fold cross validation package
library(modelr)
library(plyr)
#used to plot correlation
library(corrplot)
library(ggplot2)
#XXXXXXXXXXXXXXXX The CUSTOM MODEL FITTING FUNCTION XXXXXXXXXXXXXXXXXXXXX
fit_model <- function(k, folding,f,scene){
accuracy <- matrix(nrow = k, ncol = 3)
for(i in 1:k){
train.data <- as.data.frame(folding$train[[i]])
test.data <- as.data.frame(folding$test[[i]])
n_cols <- ncol(train.data) - 1
#table(train.data$classnotckd)
nn <- neuralnet(f, data=train.data, hidden=c(5,4), linear.output = T)
plot(nn)
dev.copy(png, paste("./images/fold - ",i , scene , " model.png"))
dev.off()
#Test the model
pr.nn <- compute(nn,test.data[,1:n_cols])
#Confusion Matrix & Classification error. Saved as png
prediction <- pr.nn$net.result
classified_prediction <- ifelse(prediction>0.5,'No CKD','CKD')
test_data <- ifelse(test.data$classnotckd == 1, 'No CKD','CKD')
cm <- confusionMatrix(factor(classified_prediction), factor(test_data))
accuracy[i,1] <- cm$overall['Accuracy']
accuracy[i,2] <- cm$overall['Kappa']
accuracy[i,3] <- cm$overall['AccuracyPValue']
confusion_matrix <- as.table(cm)
png(file = paste("./images/fold - ",i , scene , " confusion matrix.png"))
fourfoldplot(confusion_matrix)
dev.off()
#Save the merged training and test data to a file for current fold
train.data$type <- 'train'
test.data$type <- 'test'
write.csv(rbind(train.data, test.data), file = paste("./10-fold-data/",i , scene , " - fold.csv"),row.names=FALSE)
pbar$step()
}
return (accuracy)
}
#XXXXXXXXXXXXXXXX CUSTOM MODEL ACCURACY PLOT FUNCTION XXXXXXXXXXXXXXXXXXXXX
calculateAccuracy <- function(accuracy, chartTitle, k){
colnames(accuracy) <- c("Accuracy", "Kappa", "AccuracyPValue")
accuracy <- rbind(accuracy, colMeans(accuracy))
row_names <- NULL
for(i in 1:(k+1)){
row_names[i] <- ifelse(i <= k, paste("Fold ", i), 'Mean')
}
rownames(accuracy) <- c(row_names)
accuracy_results <- as.data.frame(accuracy)
#calc per centage accuracy
accuracy_results$PercentageAccuracy <- accuracy_results$Accuracy * 100
df.mean = accuracy_results %>%
mutate(ymean = mean(PercentageAccuracy))
#created as a factor to prevent inadvertent ordering of X axis
x <- factor(c(row.names(accuracy_results)), levels = c(row.names(accuracy_results)))
y <- accuracy_results$PercentageAccuracy
png(file = paste("./images/", chartTitle, '.png'))
acc_plot <- ggplot(accuracy_results, aes(x, y, fill=Accuracy)) +
geom_col() +
ggtitle(chartTitle) +
labs(caption = paste("Red line is for Average Accuracy at ", accuracy_results$PercentageAccuracy[k+1], '%')) +
ylab('% Accuracy') +
xlab('Cross Validation Folds') +
geom_errorbar(data=df.mean, aes(x, ymax = ymean, ymin = ymean),
size=0.5, linetype = "longdash", inherit.aes = F, width = 1, color = 'red')
print(acc_plot)
dev.off()
}
#XXXXXXXXXXXXXXXX CUSTOM Data Randomization function XXXXXXXXXXXXXXXXXXXXX
randomize_data <- function(data, filename){
set.seed(113)
rows <- sample(nrow(data))
data <- data[rows, ]
write.csv(data, file = filename,row.names=FALSE)
return (data)
}
data <- read.csv('transformed_imputed_data.csv')
data <- randomize_data(data, "randomized_imputed_records.csv")
aggr(data)
library(VIM)
ckd.data <-read.csv('ckd_data_converted.csv')
aggr(ckd.data)
barMiss(data)
?kNN
ckd.data <-read.csv('ckd_data_converted.csv')
attribute_labels <- as.vector(names(ckd.data))
imputable_fields <- attribute_labels[1:24]
imputed_data  <- kNN(ckd.data, k = 12, variable = imputable_fields, numFun = weightedMean)
ckd.data.kNN <- subset(imputed_data, select = age:class)
write.csv(ckd.data.kNN, file = "original_imputed_data.csv",row.names=FALSE)
imputed_data  <- kNN(ckd.data, k = 12, variable = imputable_fields, numFun = weighted.mean)
attribute_labels <- as.vector(names(ckd.data))
imputable_fields <- attribute_labels[1:24]
imputed_data  <- kNN(ckd.data, k = 12, variable = imputable_fields, numFun = weighted.mean)
ckd.data.kNN <- subset(imputed_data, select = age:class)
write.csv(ckd.data.kNN, file = "original_imputed_data.csv",row.names=FALSE)
#imputation report
png('./images/data_imputation_report.png')
aggr(imputed_data, delimiter="_imp", numbers=TRUE, prop=c(TRUE,FALSE))
dev.off()
ckd = read.csv('original_imputed_data.csv')
numeric_cols <- sapply(ckd, is.numeric)
standardize_data <- function(x){ (x - min(x))/(max(x) - min(x)) }
ckd[numeric_cols] <- lapply(ckd[numeric_cols], standardize_data)
#str(ckd)
write.csv(ckd, file = "imputed_cases_numeric_scaled.csv",row.names=FALSE)
ckd <- read.csv("imputed_cases_numeric_scaled.csv", stringsAsFactors = TRUE)
#convert factors to numeric by changing them into new columns +0/-1 removes the intercept column from the dataset
converted_to_numeric_column <- model.matrix(~age+bp+sg+al+su+rbc+pc+pcc+ba+bgr+bu+sc+sod+pot+hemo+pcv+wc+rc+htn+dm+cad+appet+pe+ane+class+0, ckd)
#save the transformed data to a file
write.csv(converted_to_numeric_column, file = "transformed_imputed_data.csv",row.names=FALSE)
data <- read.csv('transformed_imputed_data.csv')
data <- randomize_data(data, "randomized_imputed_records.csv")
# XXXXXXXXXXXXXXXXXXXXXXXXXXXX Fitting the Imputed data model XXXXXXXXXXXXXXXXXXXXXXXXXXXX
attribute_labels <- names(data)
f <- as.formula(paste("classnotckd ~", paste(attribute_labels[!attribute_labels %in% "classnotckd"], collapse = " + ")))
k <- 10
pbar <- create_progress_bar('text')
pbar$init(k)
set.seed(769)
folding <- crossv_kfold(data,k)
accuracy <- fit_model(k,folding, f, ' - Imputed Data - ')
# XXXXXXXXXXXXXXXXXXXXXXXXXXXX Model Accuracy Plot XXXXXXXXXXXXXXXXXXXXXXXXXXXX
calculateAccuracy(accuracy, "Imputed Data Accuracy Plot",k)
